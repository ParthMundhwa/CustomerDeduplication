# ğŸ§¼ Customer Deduplication (Scala + Spark)

A production-style template for **customer identity resolution and deduplication** using **Apache Spark (Scala)**.

It standardizes inputs, generates blocking keys, performs **deterministic + fuzzy matching** (Jaroâ€“Winkler + Soundex), and selects a **golden record** per cluster via **survivorship rules**.

## âœ¨ Features
- Email/phone **normalization**
- Phonetic key (**Soundex**) on last name
- **Blocking** on email / phone / soundex
- **Fuzzy** name + address (Jaroâ€“Winkler via Apache Commons Text)
- **Survivorship**: preferred source â†’ most recent â†’ most complete
- Outputs both **golden customers** and **duplicate pairs**

## ğŸ—‚ Project Structure
```
customer-deduplication-scala/
â”œâ”€ build.sbt
â”œâ”€ project/plugins.sbt          # sbt-assembly
â”œâ”€ src/
â”‚  â”œâ”€ main/scala/com/parthmundhwa/dedup/
â”‚  â”‚  â”œâ”€ Utils.scala
â”‚  â”‚  â””â”€ DedupJob.scala
â”‚  â””â”€ main/resources/rules/survivorship.yaml
â””â”€ data/
   â”œâ”€ raw/customers.csv         # sample data (with dupes)
   â””â”€ curated/.keep             # outputs written here
```

## ğŸš€ Run Locally (sbt)
```bash
# 1) run with local[*]
sbt run

# 2) build an uber-jar (fat jar)
sbt assembly

# 3) submit with spark
spark-submit --class com.parthmundhwa.dedup.DedupJob   --master local[*]   target/scala-2.12/customer-deduplication-scala-assembly-0.1.0-SNAPSHOT.jar
```

Outputs:
- `data/curated/customers_deduped.csv`
- `data/curated/duplicate_pairs.csv`

## âš™ï¸ Configure
Tune thresholds, blocking keys, and survivorship priorities in:
```
src/main/resources/rules/survivorship.yaml
```

## ğŸ“Œ Notes
- For true connected components clustering, add GraphFrames (not included here) or run a transitive closure job.
- Replace sample data with your feeds; wire into Airflow or Spark on Kubernetes for production.

## ğŸªª License
MIT Â© 2025 Parth Mundhwa
